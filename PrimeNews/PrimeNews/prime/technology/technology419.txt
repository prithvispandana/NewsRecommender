We previously discussed the Automatic Keyword Extraction via the Elasticsearch 5.0 Ingest API. Here, we will go over what is an Ingest Node, what type of operations one can perform, and show a specific example starting from scratch to parse and display CSV data using Elasticsearch and Kibana.&nbsp;
Indexing documents into the cluster can be done in a couple of ways:&nbsp;
Logstash to read from source and send documents to the clusterFilebeat to read a log file, send documents to Kafka, let Logstash connect to Kafka and transform the log event and then send those documents to the clustercurl and the Bulk API to index a pre-formatted fileJava Transport Client from within a custom application
Before Elasticsearch version 5.x, however, there were mainly two ways to transform the source data to the document&nbsp;(Logstash filters or you had to do it yourself).
In Elasticsearch 5.x the concept of the Ingest Node has been introduced. It is just a node in the cluster like any other but with the ability to create a pipeline of processors that can modify incoming documents. The most frequently used Logstash filters have been implemented as processors.
We can use ingest node to pre-process documents before the actual indexing takes place. This pre-processing happens by an ingest node that intercepts bulk and index requests, applies the transformations, and then passes the documents back to the index or bulk APIs.